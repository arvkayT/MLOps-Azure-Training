{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f27ad59-25b0-4a67-9556-cd407d6166a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 1 (chapters 1 & 2 from Spark:The definitive guide)\n",
    "\n",
    "1. **What is bigdata?**\n",
    "\n",
    "    The definition of big data is data that contains greater variety, arriving in increasing volumes and with more velocity. This is also known as the three Vs. Put simply, big data is larger, more complex data sets, especially from new data sources. These data sets are so voluminous that traditional data processing software just can’t manage them. But these massive volumes of data can be used to address business problems you wouldn’t have been able to tackle before. \n",
    "     \n",
    "2. **Why spark?**\n",
    "\n",
    "    Cluster computing holds tre‐mendous potential. At every organization that uses MapReduce, brand new applications could be built using the existing data. However, the MapReduce engine made it both challenging and inefficient to build large applications. To address this problem, the Spark team first designed an API based on functional programming that could succinctly express multistep applications. This new engine could also perform efficient, in-memory data sharing across computation steps. \n",
    "\n",
    "3. **What is spark?**\n",
    "\n",
    "    Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching and optimized query execution for fast queries against data of any size. Simply put, Spark is a fast and general engine for large-scale data processing.\n",
    "\n",
    "4. **Internals of spark?**\n",
    "\n",
    "    Spark Applications consist of a **driver** process and a set of **executor** processes. \n",
    "    \n",
    "    The **driver** process runs your main() function, sits on a node in the cluster, and is responsible for three things: maintaining information about the Spark Application; responding to a user’s program or input; and analyzing, distributing, and scheduling work across the executors (discussed momentarily). The driver process is absolutely essential it’s the heart of a Spark Application and maintains all relevant information during the lifetime of the application. \n",
    "    \n",
    "    The **executors** are responsible for actually carrying out the work that the driver assigns them. This means that each executor is responsible for only two things: executing code assigned to it by the driver, and reporting the state of the computation on that executor back to the driver node.\n",
    "\n",
    "5. **Highlevel API of spark?**\n",
    "\n",
    "    - **Sparksession**: We can control our Spark Application through a driver process called the SparkSession. The SparkSession instance is the way Spark executes user-defined manipulations across the cluster.\n",
    "    - **Dataframe**: A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list that defines the columns and the types within those columns is called the schema. \n",
    "    - **Partitions**: To allow every executor to perform work in parallel, Spark breaks up the data into chunks called partitions. A partition is a collection of rows that sit on one physical machine in your cluster. \n",
    "    - **Transformation**: To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want. These instructions are called transformations.\n",
    "    - **Actions**: Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An action instructs Spark to compute a result from a series of transformations. \n",
    "    - **Lazy Evaluation**: Lazy evaulation means that Spark will wait until the very last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when you express some operation, you build up a plan of transformations that you would like to apply to your source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8783e9fe-a174-4b61-a63d-b92859b176f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 2 (chapter 4, 5)\n",
    "\n",
    "1. **Structured API**\n",
    "    - **SQL**: One use of Spark SQL is to execute SQL queries. Spark SQL can also be used to read data from an existing Hive installation. When running SQL from within another programming language the results will be returned as a Dataset/DataFrame which I will elaborate below.\n",
    "    - **Dataset**: A Dataset is a distributed collection of data. Dataset provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).\n",
    "    - **Dataframes**: A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "2. **Basic Structured Operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f240a50-92d1-4a5b-94e0-3a8fcd0024ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **Schemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a88921f4-2b8d-4717-912d-b8e5f7bf1c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a75599-872a-4d9a-8a9d-655e5a9ce3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **Columns and Expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ffec36-7ded-4ae7-bce6-9a21376d38fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"count\")\n",
    "column(\"count\")\n",
    "\n",
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")\n",
    "\n",
    "spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e6713c-5426-4a22-99db-3fb6f5a7fc56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **Records and Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d60dff8-444c-47c1-b434-f09f72e6c6d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\")\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d646e33a-f8ce-4dd7-b7eb-70d45c5d7232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. **Create Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69fe833-3f60-4b45-87ba-25bc545f58a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 1"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)\n",
    "\n",
    "myRow[0]\n",
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfb38f6-2f90-4c98-9369-9cf04371ec6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. **Dataframe Transformation**\n",
    "\n",
    "Several types of transformations are possible which include:\n",
    "- Adding columns or rows\n",
    "- Removing rows or columns\n",
    "- Transforming a row into a column and vice versa\n",
    "- Changing the order of rows based on values in columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d401cac2-4104-496a-966d-bccd14914ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. **Creating Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916a2b0b-a5f2-44e3-be51-5ab5bd91f1f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\")\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    " StructField(\"some\", StringType(), True),\n",
    " StructField(\"col\", StringType(), True),\n",
    " StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc3889f7-421f-4ce9-b9b0-e5cb19bd9bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. **select and selectExpr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df76780-1d47-45d2-b28b-b371748d9565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n+-----------------+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+-----------------+\n|    United States|    United States|    United States|\n|    United States|    United States|    United States|\n+-----------------+-----------------+-----------------+\nonly showing top 2 rows\n\n+-------------+\n|  destination|\n+-------------+\n|United States|\n|United States|\n+-------------+\nonly showing top 2 rows\n\n+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n+-------------+-----------------+\n|newColumnName|DEST_COUNTRY_NAME|\n+-------------+-----------------+\n|United States|    United States|\n|United States|    United States|\n+-------------+-----------------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"dbfs:/user/hive/warehouse/ak_2015_summary\")\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    " expr(\"DEST_COUNTRY_NAME\"),\n",
    " col(\"DEST_COUNTRY_NAME\"),\n",
    " column(\"DEST_COUNTRY_NAME\"))\\\n",
    " .show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
    " .show(2)\n",
    "\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\n",
    " \"*\", # all original columns\n",
    " \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    " .show(2)\n",
    "\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "488a132d-d0ab-4f0d-9d5c-e8b7bdc59814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. **Adding columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9935ec-fc27-48e7-99f9-7ac54bcd271f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|        1|\n|    United States|            Croatia|    1|        1|\n+-----------------+-------------------+-----+---------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    " .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff22b88d-46b9-4a69-9aa7-080d32e3a894",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. **Renaming columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79103e21-a128-416e-9ff7-3bce46e4936f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: ['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6eb7778-3548-482f-9882-d7307c661a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "10. **Changing column's type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9e750d-c296-4c7a-9bd8-f6b643c0b175",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[36]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e61d8f-1e69-4b1d-ab70-5f0781e4100d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "11. **Filtering rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f60873-c2ee-4ab5-a19d-c36ea99646a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|          Singapore|    1|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)\n",
    "\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
    " .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "886a8303-3cc0-4e33-ae42-8f023457ef1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "12. **Getting unique rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8703b47-19b3-4676-80af-296e162f5350",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: 125"
     ]
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7506f77-b588-4e1e-bf46-4dbd1614d9be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "13. **Sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a97497-d440-4d3d-a95e-70b9429997c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|          Moldova|      United States|    1|\n|    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n+-----------------+-------------------+------+\nonly showing top 2 rows\n\nOut[45]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "\n",
    "spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\")\\\n",
    " .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646a8b6d-16a5-47f1-939d-59d8e2e91640",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "14. **Limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c00f63-2d0e-4896-a36d-e83f7dc41444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n|             Moldova|      United States|    1|\n+--------------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n",
    "\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4fd468e-31de-4000-9313-3491f38cc1c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "15. **Repartition and Coalesce**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2cbe19-36f5-47cd-bef4-33230a5c5dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[51]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() # 1\n",
    "\n",
    "df.repartition(5)\n",
    "\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b569479-e5be-4e7a-a8c7-8fe289ea8e76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 3 (chapter 7)\n",
    "\n",
    "1. **Aggregation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7e2dbc-2190-44bb-9b57-8f975b855e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a4d026-899b-4019-9a07-3b7dcbd22953",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .load(\"dbfs:/user/hive/warehouse/online_retail_dataset\")\\\n",
    " .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "df.count() == 541909\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show() # 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50762e17-f6ea-4eea-90ac-98fdcee0c078",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **countDistinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec7484a-8743-4110-8b20-070e3384e1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show() # 4070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4863ae2c-2803-419b-af0e-689972abe6e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **first and last**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036710b5-d921-4cd7-96f9-b1a3bde64cf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n|first(StockCode)|last(StockCode)|\n+----------------+---------------+\n|          85123A|          22138|\n+----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9504402-8be2-4c67-96b2-6172a5f19569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. **min and max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e41000a-b0e0-4deb-a3b3-afb22488ca80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|       -80995|        80995|\n+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113050c5-5aa1-48f1-8481-7a57d0a72cd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. **sum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ab1fa7-b033-4a34-9bbd-88353ac1a8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show() # 5176450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28ec57f2-af1e-45b5-a84e-1d97d22925c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. **sumDistinct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4002a21c-8417-4bae-a2b0-775febd77749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|sum(DISTINCT Quantity)|\n+----------------------+\n|                 29310|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show() # 29310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f3b004-c0d3-42c1-aaf2-44c30e7424da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. **avg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d560da19-7bc4-4f0c-bc12-3a717a653451",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "df.select(\n",
    " count(\"Quantity\").alias(\"total_transactions\"),\n",
    " sum(\"Quantity\").alias(\"total_purchases\"),\n",
    " avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    " expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    " .selectExpr(\n",
    " \"total_purchases/total_transactions\",\n",
    " \"avg_purchases\",\n",
    " \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5b5366c-522b-437f-9aea-2ff01a8a1d96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. **grouping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19217099-47de-4770-8b89-1f954b787b54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   536415|     12838|   59|\n|   536748|     15107|    5|\n|   536788|     15061|    1|\n|   537694|     14901|   14|\n|   537796|     18061|    6|\n|   537833|     13270|    1|\n|   538872|     13097|   11|\n|   539326|     14560|   14|\n|  C539486|     18256|    4|\n|   540354|     13576|   16|\n|   540371|     14312|   15|\n|  C541222|     14299|    1|\n|   541265|     17609|  100|\n|   541847|     15443|   51|\n|   542542|     12431|   20|\n|   543009|     18041|   10|\n|  C543290|     16686|    2|\n|   543459|     12599|   14|\n|   543550|     12601|    6|\n|   543808|     17454|   29|\n+---------+----------+-----+\nonly showing top 20 rows\n\n+---------+----+---------------+\n|InvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n|   536596|   6|              6|\n|   536938|  14|             14|\n|   537252|   1|              1|\n|   537691|  20|             20|\n|   538041|   1|              1|\n|   538184|  26|             26|\n|   538517|  53|             53|\n|   538879|  19|             19|\n|   539275|   6|              6|\n|   539630|  12|             12|\n|   540499|  24|             24|\n|   540540|  22|             22|\n|  C540850|   1|              1|\n|   540976|  48|             48|\n|   541432|   4|              4|\n|   541518| 101|            101|\n|   541783|  35|             35|\n|   542026|   9|              9|\n|   542375|   6|              6|\n|  C542604|   8|              8|\n+---------+----+---------------+\nonly showing top 20 rows\n\n+---------+------------------+--------------------+\n|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n+---------+------------------+--------------------+\n|   536596|               1.5|  1.1180339887498947|\n|   536938|33.142857142857146|  20.698023172885524|\n|   537252|              31.0|                 0.0|\n|   537691|              8.15|   5.597097462078001|\n|   538041|              30.0|                 0.0|\n|   538184|12.076923076923077|   8.142590198943392|\n|   538517|3.0377358490566038|  2.3946659604837897|\n|   538879|21.157894736842106|  11.811070444356483|\n|   539275|              26.0|  12.806248474865697|\n|   539630|20.333333333333332|  10.225241100118645|\n|   540499|              3.75|  2.6653642652865788|\n|   540540|2.1363636363636362|  1.0572457590557278|\n|  C540850|              -1.0|                 0.0|\n|   540976|10.520833333333334|   6.496760677872902|\n|   541432|             12.25|  10.825317547305483|\n|   541518| 23.10891089108911|  20.550782784878713|\n|   541783|11.314285714285715|   8.467657556242811|\n|   542026| 7.666666666666667|   4.853406592853679|\n|   542375|               8.0|  3.4641016151377544|\n|  C542604|              -8.0|  15.173990905493518|\n+---------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    " count(\"Quantity\").alias(\"quan\"),\n",
    " expr(\"count(Quantity)\")).show()\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8f2da6-69e7-4823-87a7-5543b98132ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. **window functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d55d16f-c097-4b77-ab3a-0aacb55462ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n+----------+----------+--------+------------+-----------------+-------------------+\n|     12346|2011-01-18|   74215|           1|                1|              74215|\n|     12346|2011-01-18|  -74215|           2|                2|              74215|\n|     12347|2010-12-07|      36|           1|                1|                 36|\n|     12347|2010-12-07|      30|           2|                2|                 36|\n|     12347|2010-12-07|      24|           3|                3|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|      12|           4|                4|                 36|\n|     12347|2010-12-07|       6|          17|                5|                 36|\n|     12347|2010-12-07|       6|          17|                5|                 36|\n+----------+----------+--------+------------+-----------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    " .partitionBy(\"CustomerId\", \"date\")\\\n",
    " .orderBy(desc(\"Quantity\"))\\\n",
    " .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "from pyspark.sql.functions import col\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    " .select(\n",
    " col(\"CustomerId\"),\n",
    " col(\"date\"),\n",
    " col(\"Quantity\"),\n",
    " purchaseRank.alias(\"quantityRank\"),\n",
    " purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    " maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e4655ab-1098-4bba-b572-e50bfadaad2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 4 (chapter 8)\n",
    "\n",
    "1. **Joins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c51906db-edf5-41e3-bf0b-84fb30ea6e35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **Try out examples for each types of joins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cbe32d-a3c3-49a1-ab07-9ea3f01d7b53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    " (0, \"Bill Chambers\", 0, [100]),\n",
    " (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    " (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    " .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    " (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    " (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    " (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    " .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    " (500, \"Vice President\"),\n",
    " (250, \"PMC Member\"),\n",
    " (100, \"Contributor\")])\\\n",
    " .toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf33431-baea-416b-8552-e4f1b0665965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Inner Joins\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]\n",
    "\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "\n",
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e62f32b-72f2-4e10-9b5b-5bcee2d33605",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Outer Joins\n",
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1949a6b1-e4ab-482c-8611-03e40d837d07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Outer Joins\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c914288-e07b-4c01-9b75-1475fb076db8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Right Outer Joins\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bcb06fe-3dc9-4b0e-a631-dd946e7b3494",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n| id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n|  0|Masters|School of Informa...|UC Berkeley|\n|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n+---+-------+--------------------+-----------------+\n| id| degree|          department|           school|\n+---+-------+--------------------+-----------------+\n|  0|Masters|School of Informa...|      UC Berkeley|\n|  1|  Ph.D.|                EECS|      UC Berkeley|\n|  0|Masters|      Duplicated Row|Duplicated School|\n+---+-------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Semi Joins\n",
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n",
    "\n",
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    " (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95932613-b4ca-49ee-967c-85502ed19070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n| id| degree|department|     school|\n+---+-------+----------+-----------+\n|  2|Masters|      EECS|UC Berkeley|\n+---+-------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Left Anti Joins\n",
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104836ca-c32c-40f5-8619-96d7c2ee5d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cross (Cartersian) Joins\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n",
    "\n",
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6e7091-0c39-4d5c-b535-ae8f6863f75c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **Handling Duplicate column names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0458dfef-5739-4845-9d88-9c458c6d5437",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|graduate_program|\n+----------------+\n|               0|\n|               1|\n|               1|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n",
    "joinExpr =  gradProgramDupe.select(\"graduate_program\")\n",
    "\n",
    "person.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8031c7-3f14-4de0-96d1-b8e5cc1f5125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **How spark performs joins**\n",
    "\n",
    "    - **Big table-to-big table**: When you join a big table to another big table, you end up with a shuffle join. In a shuffle join, every node talks to every other node and they share data according to which node has a certain key or set of keys (on which you are joining). These joins are expensive because the network can become congested with traffic, especially if your data is not partitioned well.\n",
    "    - **Big table-to-small table**: When the table is small enough to fit into the memory of a single worker node, with some breathing room of course, we can optimize our join. Here we will replicate our small DataFrame onto every worker node in the cluster (be it located on one machine or many). This does prevent us from performing the all-to-all communication during the entire join process. This means that joins will be performed on every single node individually, making CPU the biggest bottleneck.\n",
    "    - **Little table-to-little table**: When performing joins with small tables, it’s usually best to let Spark decide how to join them. You can always force a broadcast join if you’re noticing strange behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bddda2c-7b30-4683-85b3-7c78068f6899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 5 (chapter 9)\n",
    "\n",
    "1. **Datasources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6caf83-69fa-4f55-8e9c-8a7eed47f06f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **Basics of reading data**\n",
    "\n",
    "The foundation for reading data in Spark is the *DataFrameReader*. We access this through the SparkSession via the read attribute: \\\n",
    "*spark.read*\n",
    "\n",
    "Here’s an example of the overall layout: \\\n",
    "*spark.read.format(\"csv\") \\\n",
    " .option(\"mode\", \"FAILFAST\") \\\n",
    " .option(\"inferSchema\", \"true\") \\\n",
    " .option(\"path\", \"path/to/file(s)\") \\\n",
    " .schema(someSchema) \\\n",
    " .load()*\n",
    "\n",
    "The default read mode is *permissive*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6caed998-d066-481c-b5b3-4c53e2a34ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. **Basics of write data**\n",
    "\n",
    "The core structure for writing data is as follows: \\\n",
    "*DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()*\n",
    "\n",
    "Example: \\\n",
    "*dataframe.write.format(\"csv\") \\\n",
    " .option(\"mode\", \"OVERWRITE\") \\\n",
    " .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    " .option(\"path\", \"path/to/file(s)\") \\\n",
    " .save()*\n",
    "\n",
    "The default write mode is *errorIfExists*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76007a9-b42c-4261-aec1-96d5b29fb2ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. **CSV files - reading, writing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51978120-cb9b-4eaa-bc77-e719bceff4a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading a csv file\n",
    "csvFile = spark.read.format(\"delta\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .option(\"mode\", \"FAILFAST\")\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .load(\"dbfs:/user/hive/warehouse/online_retail_dataset\")\n",
    "\n",
    "# Writing a csv file\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\")\\\n",
    " .save(\"/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19f7c27e-51f4-4afd-aa14-7aa3f00b37b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "4. **REading and writing json files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a50a36b-316d-45f1-8389-a56761c301ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Reading a JSON file\n",
    "spark.read.format(\"delta\").option(\"mode\", \"FAILFAST\")\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .load(\"dbfs:/user/hive/warehouse/ak_2015_summary\").show(5)\n",
    "\n",
    "# Writing a JSON file\n",
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7563514-826d-4e8d-b5d6-10f779af00bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "5. **Parquet files - important**\n",
    "\n",
    "Parquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. It provides columnar compression, which saves storage space and allows for reading individual columns instead of\n",
    "entire files. It is a file format that works exceptionally well with Apache Spark and is in fact the default file format.\n",
    "Here’s how to specify Parquet as the read format: \\\n",
    "*spark.read.format(\"parquet\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46dfc851-fc95-42ca-93bf-6aac92805bad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6. **Reading and Writing parquet files**\n",
    "\n",
    "*Note: Databricks doesn't support uploading parquet files as a dataset option so moving ahead with JSON file option only*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e388f2-469c-438d-a6c7-dc307574b8e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Reading a file\n",
    "spark.read.format(\"delta\") \\\n",
    ".load(\"dbfs:/user/hive/warehouse/ak_2015_summary\").show(5)\n",
    "\n",
    "# Writing a file\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    " .save(\"/tmp/my-parquet-file.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18366404-04a3-4eed-b02d-b479675c9098",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. **orc - optional**\n",
    "\n",
    "*Note: Databricks doesn't support uploading ORC files as a dataset option so skipping this as it is optional*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ae4b59-10f1-49ca-9640-1e3902911f5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. **Splittable File Types and COmpression**\n",
    "\n",
    "Certain file formats are fundamentally “splittable.” This can improve speed because it makes it possible for Spark to avoid reading an entire file, and access only the parts of the file necessary to satisfy your query. \\\n",
    "In conjunction with this is a need to manage compression. Not all compression schemes are splittable. How you store your data is\n",
    "of immense consequence when it comes to making your Spark jobs run smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcaba36a-54a2-428c-90ae-759df424e97f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. **Managing File size**\n",
    "\n",
    "Managing file sizes is an important factor not so much for writing data but reading it later on. Spark especially does not do well with\n",
    "small files, although many file systems (like HDFS) don’t handle lots of small files well, either. You might hear this referred to as the “small file problem.” \\\n",
    "Spark 2.2 introduced a new method for controlling file sizes in a more automatic way. You can use the **maxRecordsPerFile** option and specify a number of your choosing.  For example, if you set an option for a writer as **df.write.option(\"maxRecordsPerFile\", 5000)**, Spark will ensure that files will contain at most 5,000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35e5f85-8d16-4153-bc49-e0208fffc982",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Day 6 (chapter 10)\n",
    "\n",
    "**Entire chapter of Spark SQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc27d6e-9654-453d-8956-f36c93b3634c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Spark’s Programmatic SQL Interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8c4196-d9c5-4ccc-83a8-b41429571ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|(1 + 1)|\n+-------+\n|      2|\n+-------+\n\nOut[134]: 12"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()\n",
    "\n",
    "spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/ak_2015_summary\")\\\n",
    " .createOrReplaceTempView(\"some_sql_view\") # DF => SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count)\n",
    "FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\\\n",
    " .where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    " .count() # SQL => DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38267637-433f-4856-a5d1-40e0dd5bb1fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Creating Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec851d9-7a07-445a-a368-b733b6114695",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE flights (\n",
    " DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path 'dbfs:/user/hive/warehouse/ak_2015_summary')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1ce434-0d5d-42e5-92ce-ce618dc15037",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[142]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE flights_csv (\n",
    " DEST_COUNTRY_NAME STRING,\n",
    " ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\",\n",
    " count LONG)\n",
    "USING csv OPTIONS (header true, path 'dbfs:/user/hive/warehouse/ak_2015_summary')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce63c709-6d66-4308-9187-eb7c3c59fdc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SET spark.databricks.delta.formatCheck.enabled=false\"\"\")\n",
    "spark.sql(\"\"\"CREATE TABLE flights_from_select AS SELECT * FROM flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceda02d7-a6b5-4924-b742-2db1f24943df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS flights_from_select\n",
    " AS SELECT * FROM flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8be720-198e-40e7-a57b-1c016172a1a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)\n",
    "AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147084cc-d124-46da-b1da-0bf7d53ab1eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Creating External Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1439b16d-e8f1-41ef-aa1f-c112388bffa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[146]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          CREATE EXTERNAL TABLE hive_flights (\n",
    " DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'dbfs:/user/hive/warehouse/ak_2015_summary'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd47765d-2f60-4bca-bfe6-aa05ea02ff53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE EXTERNAL TABLE hive_flights_2\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "LOCATION '/data/flight-data-hive/' AS SELECT * FROM flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a283cd-b062-4c1e-bd5a-ab33605cda0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Inserting into Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06061f76-3743-4503-a64b-0345e9110bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO flights_from_select\n",
    " SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57473b3b-3029-468b-9268-c29ed77873e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Describing Table Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba46834c-5bc5-41d3-afd2-1510623ffda9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: DataFrame[col_name: string, data_type: string, comment: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"DESCRIBE TABLE flights_csv\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fff3613-0070-4873-a57f-bf63d6692ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: DataFrame[partition: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW PARTITIONS partitioned_flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54bbb25d-9585-4d13-beaa-c6e9dca3d3a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Refreshing Table Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4853a041-6481-4ca6-b581-291594d8f2ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"REFRESH table partitioned_flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed663f1-683e-43d0-bdb4-fdd199159eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"MSCK REPAIR TABLE partitioned_flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19292de4-d575-49e2-bcc5-f37856a52ccc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dropping Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1e7c23-2858-4121-8537-e8add2bd1670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE flights_csv\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2d2510-ebe9-430b-866e-ed777c58d7a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS flights_csv\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188d1e6d-3d77-40c9-bcda-2a7f28c75e70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Caching Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b293102c-5428-4516-8147-0ffee1b576c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CACHE TABLE flights\"\"\")\n",
    "spark.sql(\"\"\"UNCACHE TABLE FLIGHTS\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f158dd99-663d-41a9-a9fc-7af57e0a1ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Creating Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445135a9-f9b2-45a3-80f1-19db635c6bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: DataFrame[database: string, tableName: string, isTemporary: boolean]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE VIEW just_usa_view AS\n",
    " SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE TEMP VIEW just_usa_view_temp AS\n",
    " SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS\n",
    " SELECT * FROM flights WHERE dest_country_name = 'United States'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SHOW TABLES\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155f1e4c-bf22-4b1c-b7a9-36baa7be8f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS\n",
    " SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM just_usa_view_temp\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcc4e6a-e825-4cba-87c5-d1701062040f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: DataFrame[plan: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"EXPLAIN SELECT * FROM just_usa_view\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"EXPLAIN SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5cc5d7-b343-4984-b5fe-00d378d2f5b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dropping Views**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd6f253-699f-4363-afe1-11c31c7f59bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP VIEW IF EXISTS just_usa_view\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3071bc0-b8f2-4a8a-88ce-d341975ed4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9575d503-bc69-4ff2-b9fb-0832661babc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: DataFrame[databaseName: string]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW DATABASES\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "602ec375-c2c1-4546-9ee0-b2a6e4a9037d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Creating Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d227757a-0b02-42e3-a43f-617089e55dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE DATABASE some_db\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1ec8367-d138-401a-93ec-17c4c16d8504",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Setting the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a573b6-3578-40d7-aae4-0e11ff05b88b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"USE some_db\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SHOW tables\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM default.flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f05122-80ab-41e4-86c9-148aa2f62b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT current_database()\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"USE default\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e55eb6-8c57-4476-8608-d53e542cd4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Dropping Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82cbda0-5f5d-4c46-84fc-0331eb31b11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[31]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP DATABASE IF EXISTS some_db\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Arvind's-spark-assignment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
