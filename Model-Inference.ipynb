{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e19e3d2-7818-409a-b2c6-4021b1512268",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74048.90538268468\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/24 06:34:31 INFO mlflow.tracking.fluent: Experiment with name '/SparkML' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 74048.90538268468\nStage: Log Model Pipeline - Status: Started\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/05/24 06:34:43 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2023/05/24 06:36:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/2648146100547158/e18ed90be12e48db9fd562e9a76d58ff/artifacts/model/sparkml, flavor: spark), fall back to return ['pyspark==3.3.2', 'pandas<2']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: Log Model Pipeline - Status: Complete\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "eval_rmse = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = eval_rmse.evaluate(predDF)\n",
    "\n",
    "print(rmse)\n",
    "\n",
    "registry_uri=\"databricks://rmr:rmr\"\n",
    "\n",
    "# Use ML Flow to register the experiments in the local registry and register the trained model in the shared registry\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "mlflow.set_experiment(\"/SparkML\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"SparkML-Test\") as run:\n",
    "\n",
    "  # Log the algorithm parameter num_trees to the run\n",
    "  mlflow.log_param('num_trees', 10)\n",
    "\n",
    "\n",
    "  # Train model with Training Data\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "  #Scoring the test data\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  \n",
    "  #Evaluate the accuracy metrics\n",
    "  eval_rmse = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "  rmse = eval_rmse.evaluate(predDF)\n",
    "\n",
    "  print('RMSE is', str(rmse))\n",
    "  mlflow.log_param('rmse', rmse)\n",
    "  \n",
    "\n",
    "  # Log model\n",
    "  print(\"Stage: Log Model Pipeline - Status: Started\")\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "  print(\"Stage: Log Model Pipeline - Status: Complete\")\n",
    "\n",
    "  run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC \n",
    "# MAGIC Register the model in the shared registry\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mlflow.set_registry_uri(registry_uri)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Register the mode with name: SparkModel\n",
    "# run_id = run.info.run_id\n",
    "# model_uri = f\"runs:/{run_id}/model\"\n",
    "# model_details = mlflow.register_model(model_uri=model_uri, name=\"SparkModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44da68e4-37ee-4e61-8284-ed818c45d2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model-Inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
